{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import es_core_news_sm\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np\n",
    "import regex\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EntrenarModelo(oraciones,NombreModelo):\n",
    "    model = Word2Vec(oraciones,vector_size=1250, window=2, min_count=1)\n",
    "    model.save(NombreModelo)\n",
    "    \n",
    "def CargarModelo(NombreModelo):\n",
    "   modelo = Word2Vec.load(NombreModelo)\n",
    "   vocabulario = [term for term in modelo.wv.key_to_index]  \n",
    "   return(modelo,vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObtenerEmbeddingOracion(modelo, oracion):\n",
    "   Lista_vectores = []\n",
    "   for w in nltk.sent_tokenize(oracion, language=\"spanish\"):\n",
    "       # Verificar que la palabra w exista en el modelo\n",
    "       try:\n",
    "           modelo.wv[w]\n",
    "       except KeyError:\n",
    "           continue\n",
    "       # Obtener vector de la palabra\n",
    "       vec = modelo.wv[w]\n",
    "       Lista_vectores.append(vec)\n",
    "   embedding_palabras = np.array(Lista_vectores)\n",
    "   if (len(embedding_palabras) > 0):\n",
    "        embedding_oracion = embedding_palabras.mean(axis=0)\n",
    "   else:\n",
    "        embedding_oracion = np.zeros(modelo.vector_size)\n",
    "   return(embedding_oracion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrearCorpus(path):\n",
    "  directorio = os.listdir(path)\n",
    "  corpus = []\n",
    "  doc_id = []  \n",
    "  for filename  in directorio:\n",
    "     texto = open(path+filename,'r',encoding=\"UTF-8\").read()\n",
    "     corpus.append(texto)\n",
    "     doc_id.append(filename)\n",
    "  return(corpus,doc_id)\n",
    "\n",
    "def PreProcesarOraciones(textos):\n",
    "    texto_limpio = []\n",
    "    for texto in textos:\n",
    "        if len(texto) != 0:\n",
    "            texto_limpio.append(nltk.sent_tokenize(texto, language=\"spanish\"))\n",
    "    return texto_limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrearDiccionario(lista,claves):\n",
    "   dicc = {}\n",
    "   for  v in range(0,len(claves)):\n",
    "      dicc[claves[v]] = lista[v]\n",
    "   return(dicc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"DiscursosOriginales/\"\n",
    "nlp          = es_core_news_sm.load()\n",
    "corpus,docID = CrearCorpus(PATH)\n",
    "oraciones = PreProcesarOraciones(corpus)\n",
    "CorpusConClave  = CrearDiccionario(corpus,docID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "EntrenarModelo(oraciones,'mi_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo, vocabulario = CargarModelo('mi_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018361229449510574\n"
     ]
    }
   ],
   "source": [
    "doc1 = CorpusConClave['73117.txt']\n",
    "doc2 = CorpusConClave['136426.txt']\n",
    "vec1 = ObtenerEmbeddingOracion(modelo, doc1)\n",
    "vec2 = ObtenerEmbeddingOracion(modelo, doc2)\n",
    "\n",
    "similitud = 1-cosine(vec2, vec1)\n",
    "print(similitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar oraciones por puntaje\n",
    "def centroide(modelo, oraciones):\n",
    "    embedding = []\n",
    "    for oracion in oraciones:\n",
    "        embedding.append(ObtenerEmbeddingOracion(modelo, oracion))\n",
    "    return np.mean(embedding, axis=0)\n",
    "def OrdenarOraciones(oraciones):\n",
    "    puntaje = []\n",
    "    centroidee = centroide(modelo, oraciones)\n",
    "    for oracion in oraciones:\n",
    "        puntaje.append(cosine(ObtenerEmbeddingOracion(modelo, oracion),centroidee))\n",
    "    oraciones.sort(key=lambda oracion: cosine(ObtenerEmbeddingOracion(modelo, oracion),centroidee), reverse=True)\n",
    "    return oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_resumen(O,N,P,U):\n",
    "    O = OrdenarOraciones(O)\n",
    "    largo = 1\n",
    "    Resumen = []\n",
    "    M = len(O)\n",
    "    for i in range(M):\n",
    "        if (largo > N): return Resumen\n",
    "        Vo = ObtenerEmbeddingOracion(modelo, O[i])\n",
    "        incluirOracion = False\n",
    "        for j in range(largo):\n",
    "            Vr = ObtenerEmbeddingOracion(modelo, Resumen[j])\n",
    "            Sim = 1-cosine(Vo, Vr)\n",
    "            if Sim > U and (O[i] not in Resumen):\n",
    "                incluirOracion = True\n",
    "        if incluirOracion == True:\n",
    "            Resumen[largo] = O[i]\n",
    "            largo+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('jupy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f53e49ab9d0d5819243b2b9e291675436d723b4e069eaa0e7e3fc4c332cba84a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
