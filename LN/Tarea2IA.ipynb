{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import es_core_news_sm\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np\n",
    "import regex\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para entrenar el modelo\n",
    "def EntrenarModelo(oraciones,NombreModelo):\n",
    "    # vector_size parece establizar los resultados a partir de 100, por lo que empleamos un size=150 para darle un margen de error a dicha observacion\n",
    "    model = Word2Vec(oraciones,vector_size=150, window=50, min_count=1)\n",
    "    model.save(NombreModelo)\n",
    "# Funcion para cargar un modelo ya existente\n",
    "def CargarModelo(NombreModelo):\n",
    "   modelo = Word2Vec.load(NombreModelo)\n",
    "   vocabulario = [term for term in modelo.wv.key_to_index]  \n",
    "   return(modelo,vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el emebdding de un TEXTO\n",
    "def ObtenerEmbeddingOracion(modelo, oracion):\n",
    "   Lista_vectores = []\n",
    "   for w in nltk.word_tokenize(oracion, language=\"spanish\"):\n",
    "       # Verificar que la oracion w exista en el modelo\n",
    "       try:\n",
    "           modelo.wv[w]\n",
    "       except KeyError:\n",
    "           continue\n",
    "       # Obtener vector de la oracion\n",
    "       vec = modelo.wv[w]\n",
    "       Lista_vectores.append(vec)\n",
    "   embedding_palabras = np.array(Lista_vectores)\n",
    "   if (len(embedding_palabras) > 0):\n",
    "        embedding_oracion = embedding_palabras.mean(axis=0)\n",
    "   else:\n",
    "        embedding_oracion = np.zeros(modelo.vector_size)\n",
    "   return(embedding_oracion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para crear el corpus\n",
    "def CrearCorpus(path):\n",
    "  directorio = os.listdir(path)\n",
    "  corpus = []\n",
    "  doc_id = []  \n",
    "  for filename  in directorio:\n",
    "     texto = open(path+filename,'r',encoding=\"UTF-8\").read()\n",
    "     corpus.append(texto)\n",
    "     doc_id.append(filename)\n",
    "  return(corpus,doc_id)\n",
    "# Preprocesamiento de textos\n",
    "def PreProcesarTextos(textos):\n",
    "    texto_limpio = []\n",
    "    # Para cada texto tokenizar por oraciones\n",
    "    for texto in textos:\n",
    "        if len(texto) != 0:\n",
    "            texto_limpio.append(nltk.word_tokenize(texto, language=\"spanish\"))\n",
    "    return texto_limpio\n",
    "def PreProcesarTextos2(textos):\n",
    "    texto_limpio = []\n",
    "    # Para cada texto tokenizar por oraciones\n",
    "    for texto in textos:\n",
    "        if len(texto) != 0:\n",
    "            texto_limpio.append(nltk.sent_tokenize(texto, language=\"spanish\"))\n",
    "    return texto_limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrearDiccionario(lista,claves):\n",
    "   dicc = {}\n",
    "   for  v in range(0,len(claves)):\n",
    "      dicc[claves[v]] = lista[v]\n",
    "   return(dicc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"DiscursosOriginales/\"\n",
    "# Creamos el corpus a partir de todos los textos en el directiorio PATH\n",
    "corpus,docID = CrearCorpus(PATH)\n",
    "# Se divide cada texto en oraciones\n",
    "oraciones = PreProcesarTextos(corpus)\n",
    "CorpusConClave  = CrearDiccionario(corpus,docID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo con los textos tokenizados por oracion y lo llamamos mi_word2vec\n",
    "EntrenarModelo(oraciones,'mi_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos el modelo y el vocabulario\n",
    "modelo, vocabulario = CargarModelo('mi_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7110172510147095\n"
     ]
    }
   ],
   "source": [
    "doc1 = CorpusConClave['85792.txt']\n",
    "doc2 = CorpusConClave['86506.txt']\n",
    "vec1 = ObtenerEmbeddingOracion(modelo, doc1)\n",
    "vec2 = ObtenerEmbeddingOracion(modelo, doc2)\n",
    "\n",
    "similitud = 1-cosine(vec1,vec2)\n",
    "print(similitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar oraciones por puntaje\n",
    "def centroide(modelo, oraciones):\n",
    "    embedding = []\n",
    "    for oracion in oraciones:\n",
    "        embedding.append(ObtenerEmbeddingOracion(modelo, oracion))\n",
    "    return np.mean(embedding, axis=0)\n",
    "def OrdenarOraciones(oraciones):\n",
    "    puntaje = []\n",
    "    centroidee = centroide(modelo, oraciones)\n",
    "    oraciones.sort(key=lambda oracion: cosine(ObtenerEmbeddingOracion(modelo, oracion), centroidee))\n",
    "    return oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo summarization imlementado en python\n",
    "def generar_resumen(O,N,P,U):\n",
    "    O = OrdenarOraciones(O)\n",
    "    largo = 1\n",
    "    Resumen = [\"\"]*(N+1)\n",
    "    M = len(O)\n",
    "    for i in range(M):\n",
    "        if (largo > N): return Resumen[1:N+1]\n",
    "        Vo = ObtenerEmbeddingOracion(modelo, O[i])\n",
    "        incluirOracion = False\n",
    "        for j in range(largo):\n",
    "            try:\n",
    "                Vr = ObtenerEmbeddingOracion(modelo,Resumen[j])\n",
    "            except KeyError:\n",
    "                Vr = np.zeros(len(Vo))\n",
    "            Sim = 1-cosine(Vo, Vr)\n",
    "            if Sim > U and (O[i] not in Resumen):\n",
    "                incluirOracion = True\n",
    "        if incluirOracion == True:\n",
    "            Resumen[largo] = O[i]\n",
    "            largo+=1\n",
    "    return Resumen[1:N+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sólo juntos podemos dar poder a lo local, dar voz a las diferentes necesidades de nuestra gente, dignifcar el trabajo y la democracia.',\n",
       " 'Yo soy hija de la educación pública, y mi compromiso es que en Chile todos tengamos esas mismas oportunidades.',\n",
       " '¡Un programa que compromete una gran Reforma Tributaria, que consagrará el principio de que quienes tienen más, contribuyan con más al bienestar de todas y todos!',\n",
       " 'Y también de un país que trabaje incansablemente para que la dignidad y el respeto de los derechos sean la regla para todos y todas.',\n",
       " '¡Es hora de iniciar juntos ese camino hacia una nación desarrollada y justa, moderna y tolerante, próspera e inclusiva que todos nos merecemos!',\n",
       " 'Porque tenemos urgencia, debemos trabajar con unidad, con generosidad y con compromiso, no por los intereses propios, sino por el bien común.',\n",
       " '¡Un programa que se compromete con el medio ambiente, con ciudades y barrios amables, con regiones que sean protagonistas de su propio desarrollo!',\n",
       " 'Y ustedes me acompañaron en esa despedida, y hoy vuelven a estar conmigo aquí, en ésta, la casa de los Presidentes y las Presidentas de Chile.',\n",
       " 'Esa es la sociedad que juntos hemos imaginado.',\n",
       " 'Una sociedad donde los desafíos del crecimiento demandan no sólo eficiencia, sino también un acuerdo político y humano con inclusión y sustentabilidad.']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generamos un resumen de un texto aleatorio\n",
    "# https://www.bacheletpresidente.cl/discursos/primer-discurso-segundo-mandato/\n",
    "text = open('new.txt', encoding='UTF-8').read()\n",
    "oraciones = PreProcesarTextos2([text])[0]\n",
    "resumen = generar_resumen(oraciones, 10, 0,0.2)\n",
    "resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('jupy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f53e49ab9d0d5819243b2b9e291675436d723b4e069eaa0e7e3fc4c332cba84a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
