{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "import es_core_news_sm\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np\n",
    "import regex\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para entrenar el modelo\n",
    "def EntrenarModelo(oraciones,NombreModelo):\n",
    "    model = Word2Vec(oraciones,vector_size=1250, window=20, min_count=1)\n",
    "    model.save(NombreModelo)\n",
    "# Funcion para cargar un modelo ya existente\n",
    "def CargarModelo(NombreModelo):\n",
    "   modelo = Word2Vec.load(NombreModelo)\n",
    "   vocabulario = [term for term in modelo.wv.key_to_index]  \n",
    "   return(modelo,vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el emebdding de un TEXTO\n",
    "def ObtenerEmbeddingTexto(modelo, texto):\n",
    "   Lista_vectores = []\n",
    "   for w in nltk.sent_tokenize(texto, language=\"spanish\"):\n",
    "       # Verificar que la oracion w exista en el modelo\n",
    "       try:\n",
    "           modelo.wv[w]\n",
    "       except KeyError:\n",
    "           continue\n",
    "       # Obtener vector de la oracion\n",
    "       vec = modelo.wv[w]\n",
    "       Lista_vectores.append(vec)\n",
    "   embedding_oraciones = np.array(Lista_vectores)\n",
    "   if (len(embedding_oraciones) > 0):\n",
    "        embedding_texto = embedding_oraciones.mean(axis=0)\n",
    "   else:\n",
    "        embedding_texto = np.zeros(modelo.vector_size)\n",
    "   return(embedding_texto) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para crear el corpus\n",
    "def CrearCorpus(path):\n",
    "  directorio = os.listdir(path)\n",
    "  corpus = []\n",
    "  doc_id = []  \n",
    "  for filename  in directorio:\n",
    "     texto = open(path+filename,'r',encoding=\"UTF-8\").read()\n",
    "     corpus.append(texto)\n",
    "     doc_id.append(filename)\n",
    "  return(corpus,doc_id)\n",
    "# Preprocesamiento de textos\n",
    "def PreProcesarTextos(textos):\n",
    "    texto_limpio = []\n",
    "    for texto in textos:\n",
    "        if len(texto) != 0:\n",
    "            texto_limpio.append(nltk.sent_tokenize(texto, language=\"spanish\"))\n",
    "    return texto_limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrearDiccionario(lista,claves):\n",
    "   dicc = {}\n",
    "   for  v in range(0,len(claves)):\n",
    "      dicc[claves[v]] = lista[v]\n",
    "   return(dicc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"DiscursosOriginales/\"\n",
    "nlp          = es_core_news_sm.load()\n",
    "corpus,docID = CrearCorpus(PATH)\n",
    "oraciones = PreProcesarTextos(corpus)\n",
    "CorpusConClave  = CrearDiccionario(corpus,docID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "EntrenarModelo(oraciones,'mi_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo, vocabulario = CargarModelo('mi_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03272807598114014\n"
     ]
    }
   ],
   "source": [
    "doc1 = CorpusConClave['73117.txt']\n",
    "doc2 = CorpusConClave['136426.txt']\n",
    "vec1 = ObtenerEmbeddingTexto(modelo, doc1)\n",
    "vec2 = ObtenerEmbeddingTexto(modelo, doc2)\n",
    "\n",
    "similitud = 1-cosine(vec2, vec1)\n",
    "print(similitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar oraciones por puntaje\n",
    "def centroide(modelo, oraciones):\n",
    "    embedding = []\n",
    "    for oracion in oraciones:\n",
    "        embedding.append(modelo.wv[oracion])\n",
    "    return np.mean(embedding, axis=0)\n",
    "def OrdenarOraciones(oraciones):\n",
    "    puntaje = []\n",
    "    centroidee = centroide(modelo, oraciones)\n",
    "    oraciones.sort(key=lambda oracion: cosine(modelo.wv[oracion],centroidee), reverse=True)\n",
    "    return oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_resumen(O,N,P,U):\n",
    "    O = OrdenarOraciones(O)\n",
    "    largo = 1\n",
    "    Resumen = [\"\"]*(N+1)\n",
    "    M = len(O)\n",
    "    for i in range(M):\n",
    "        if (largo > N): return Resumen[1:N+1]\n",
    "        Vo = modelo.wv[O[i]]\n",
    "        incluirOracion = False\n",
    "        for j in range(largo):\n",
    "            try:\n",
    "                Vr = modelo.wv[Resumen[j]]\n",
    "            except KeyError:\n",
    "                Vr = np.zeros(len(Vo))\n",
    "            Sim = 1-cosine(Vo, Vr)\n",
    "            if Sim > U and (O[i] not in Resumen):\n",
    "                incluirOracion = True\n",
    "        if incluirOracion == True:\n",
    "            Resumen[largo] = O[i]\n",
    "            largo+=1\n",
    "    return Resumen[1:N+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parker/Documents/env/jupy/lib64/python3.10/site-packages/scipy/spatial/distance.py:620: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Esa entrega que han demostrado a lo largo de la historia de la Fuerza Aérea los hombres y mujeres que componen esta institución, y así lo han demostrado cada vez que la patria los ha necesitado.',\n",
       " 'Basta con mencionar, y ya lo dijo el Comandante en Jefe, la ayuda que prestaron con ocasión de tantos desastres naturales a lo largo de nuestra historia, los puentes aéreos para llevar alimentos, medicamentos y servicios básicos a miles de compatriotas que, de otra forma, tal vez no habrían logrado sobrevivir.',\n",
       " 'Las trágicas muertes de los tenientes Mery, Bello, Vidal, Marsh, el sargento Menadier, el guardiamarina Zañartu, el subteniente Wall y de tantos, fueron las semillas que lograron echar raíces profundas y que han germinado dando grandes frutos en la historia y también en el futuro de nuestra Fuerza Aérea.',\n",
       " 'Y es ese mismo compromiso y entrega que hoy hemos reconocido al entregar la condecoración “General Diego Aracena Aguilar” a seis destacados oficiales y suboficiales y civiles.',\n",
       " 'Pero lo importante es que se mantenga ese espíritu, ese compromiso que ha caracterizado a nuestra Fuerza Aérea a lo largo de su historia.',\n",
       " 'Podríamos decir que todo comenzó un 7 de marzo del año 1913, cuando el capitán Manuel Ávalos Prado protagonizó el primer vuelo militar, surcando el cielo en la entonces Chacra Lo Espejo, hoy Base Aérea “El Bosque”, a bordo de un avión bautizado con el nombre de nuestra patria: “Chile”.',\n",
       " 'Yo creo que ese compromiso y ese sentido de misión es lo que ha caracterizado a la Fuerza Aérea de Chile durante sus primeros 88 años de vida, y la va a seguir caracterizando y guiando en los años que vendrán.',\n",
       " 'Y por eso nos sentimos orgullosos de lo que ha logrado en sus primeros 88 años de vida, y nos sentimos esperanzados por lo que va a seguir aportando a la seguridad y el desarrollo de nuestro país.',\n",
       " 'En este día de alegría y celebración, quiero recordar que el hombre siempre ha querido volar: no es casualidad que desde los tiempos del mito de Dédalo e Ícaro soñaban con volar.',\n",
       " 'No me la van a ganar, ni los montes, ni las montañas, repetía, mirando los macizos de esa gran Cordillera.']"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('./DiscursosOriginales/72186.txt', encoding='UTF-8').read()\n",
    "oraciones = PreProcesarTextos([text])[0]\n",
    "resumen = generar_resumen(oraciones, 10, 0,0.2)\n",
    "resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('jupy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f53e49ab9d0d5819243b2b9e291675436d723b4e069eaa0e7e3fc4c332cba84a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
