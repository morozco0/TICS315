{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "import es_core_news_sm\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np\n",
    "import regex\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/parker/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para entrenar el modelo\n",
    "def EntrenarModelo(oraciones,NombreModelo):\n",
    "    model = Word2Vec(oraciones,vector_size=150, window=20, min_count=1)\n",
    "    model.save(NombreModelo)\n",
    "# Funcion para cargar un modelo ya existente\n",
    "def CargarModelo(NombreModelo):\n",
    "   modelo = Word2Vec.load(NombreModelo)\n",
    "   vocabulario = [term for term in modelo.wv.key_to_index]  \n",
    "   return(modelo,vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el emebdding de un TEXTO\n",
    "def ObtenerEmbeddingTexto(modelo, texto):\n",
    "   Lista_vectores = []\n",
    "   for w in nltk.word_tokenize(texto, language=\"spanish\"):\n",
    "       # Verificar que la oracion w exista en el modelo\n",
    "       try:\n",
    "           modelo.wv[w]\n",
    "       except KeyError:\n",
    "           continue\n",
    "       # Obtener vector de la oracion\n",
    "       vec = modelo.wv[w]\n",
    "       Lista_vectores.append(vec)\n",
    "   embedding_oraciones = np.array(Lista_vectores)\n",
    "   if (len(embedding_oraciones) > 0):\n",
    "        embedding_texto = embedding_oraciones.mean(axis=0)\n",
    "   else:\n",
    "        embedding_texto = np.zeros(modelo.vector_size)\n",
    "   return(embedding_texto) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para crear el corpus\n",
    "def CrearCorpus(path):\n",
    "  directorio = os.listdir(path)\n",
    "  corpus = []\n",
    "  doc_id = []  \n",
    "  for filename  in directorio:\n",
    "     texto = open(path+filename,'r',encoding=\"UTF-8\").read()\n",
    "     corpus.append(texto)\n",
    "     doc_id.append(filename)\n",
    "  return(corpus,doc_id)\n",
    "# Preprocesamiento de textos\n",
    "def PreProcesarTextos(textos):\n",
    "    texto_limpio = []\n",
    "    # Para cada texto tokenizar por oraciones\n",
    "    for texto in textos:\n",
    "        if len(texto) != 0:\n",
    "            texto_limpio.append(nltk.word_tokenize(texto, language=\"spanish\"))\n",
    "    return texto_limpio\n",
    "def PreProcesarTextos2(textos):\n",
    "    texto_limpio = []\n",
    "    # Para cada texto tokenizar por oraciones\n",
    "    for texto in textos:\n",
    "        if len(texto) != 0:\n",
    "            texto_limpio.append(nltk.sent_tokenize(texto, language=\"spanish\"))\n",
    "    return texto_limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrearDiccionario(lista,claves):\n",
    "   dicc = {}\n",
    "   for  v in range(0,len(claves)):\n",
    "      dicc[claves[v]] = lista[v]\n",
    "   return(dicc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"DiscursosOriginales/\"\n",
    "# Creamos el corpus a partir de todos los textos en el directiorio PATH\n",
    "corpus,docID = CrearCorpus(PATH)\n",
    "# Se divide cada texto en oraciones\n",
    "oraciones = PreProcesarTextos(corpus)\n",
    "CorpusConClave  = CrearDiccionario(corpus,docID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo con los textos tokenizados por oracion y lo llamamos mi_word2vec\n",
    "EntrenarModelo(oraciones,'mi_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos el modelo y el vocabulario\n",
    "modelo, vocabulario = CargarModelo('mi_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar oraciones por puntaje\n",
    "def centroide(modelo, oraciones):\n",
    "    embedding = []\n",
    "    for oracion in oraciones:\n",
    "        embedding.append(ObtenerEmbeddingTexto(modelo, oracion))\n",
    "    return np.mean(embedding, axis=0)\n",
    "def OrdenarOraciones(oraciones):\n",
    "    puntaje = []\n",
    "    centroidee = centroide(modelo, oraciones)\n",
    "    oraciones.sort(key=lambda oracion: cosine(ObtenerEmbeddingTexto(modelo, oracion), centroidee))\n",
    "    return oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo summarization imlementado en python\n",
    "def generar_resumen(O,N,P,U):\n",
    "    O = OrdenarOraciones(O)\n",
    "    largo = 1\n",
    "    Resumen = [\"\"]*(N+1)\n",
    "    M = len(O)\n",
    "    for i in range(M):\n",
    "        if (largo > N): return Resumen[1:N+1]\n",
    "        Vo = ObtenerEmbeddingTexto(modelo, O[i])\n",
    "        incluirOracion = False\n",
    "        for j in range(largo):\n",
    "            try:\n",
    "                Vr = ObtenerEmbeddingTexto(modelo,Resumen[j])\n",
    "            except KeyError:\n",
    "                Vr = np.zeros(len(Vo))\n",
    "            Sim = 1-cosine(Vo, Vr)\n",
    "            if Sim > U and (O[i] not in Resumen):\n",
    "                incluirOracion = True\n",
    "        if incluirOracion == True:\n",
    "            Resumen[largo] = O[i]\n",
    "            largo+=1\n",
    "    return Resumen[1:N+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parker/Documents/env/jupy/lib64/python3.10/site-packages/scipy/spatial/distance.py:620: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Y también quisiera no solamente reconocer en este día de alegría y celebración, con verdadero orgullo y con verdadera emoción, a esa generación de héroes y mártires, gracias a cuyas “alas enarcadas en suprema sed de cielo” -como bien dice el himno de la FACH- se ha logrado consolidar la integración de nuestro territorio y la defensa de nuestra soberanía.',\n",
       " 'Sin embargo, la formación de la Fuerza Aérea de Chile había comenzado mucho antes de que se firmara ese Decreto, gracias al esfuerzo, la creatividad y el coraje de algunos jóvenes aviadores que supieron anticiparse a su tiempo.',\n",
       " 'Pero lo cierto es que, en la historia de la aviación, Chile siempre ha contado con ese compromiso y entrega que caracterizó a sus jóvenes pioneros y héroes en el pasado.',\n",
       " 'Yo creo que ese compromiso y ese sentido de misión es lo que ha caracterizado a la Fuerza Aérea de Chile durante sus primeros 88 años de vida, y la va a seguir caracterizando y guiando en los años que vendrán.',\n",
       " 'De esa manera, sólo 10 años después del histórico vuelo a motor de los hermanos Wright, en Estados Unidos, un grupo de soñadores y jóvenes aviadores chilenos comenzó a escribir esta historia que ha estado marcada por los sueños, las hazañas y los sacrificios de nuestra querida Fuerza Aérea.',\n",
       " 'Basta con mencionar, y ya lo dijo el Comandante en Jefe, la ayuda que prestaron con ocasión de tantos desastres naturales a lo largo de nuestra historia, los puentes aéreos para llevar alimentos, medicamentos y servicios básicos a miles de compatriotas que, de otra forma, tal vez no habrían logrado sobrevivir.',\n",
       " 'Y también hoy día, 88 años después de la fundación o creación de la Fuerza Aérea de Chile, agradecer y rendir un homenaje a esos verdaderos héroes herederos, hombres y mujeres de la Fuerza Aérea de ayer, de hoy y de mañana, por haber contribuido a hacer de Chile una gran nación.',\n",
       " 'Las trágicas muertes de los tenientes Mery, Bello, Vidal, Marsh, el sargento Menadier, el guardiamarina Zañartu, el subteniente Wall y de tantos, fueron las semillas que lograron echar raíces profundas y que han germinado dando grandes frutos en la historia y también en el futuro de nuestra Fuerza Aérea.',\n",
       " 'Esa misma idea constante, esa misma determinación de volar para conectar a todos los chilenos que viven en el extremo norte, en nuestros cientos de islas, en el sur, e incluso en nuestra Antártica -y tuve el privilegio de aterrizar en la Base Glaciar Unión, dentro del Círculo Polar-, fue la que insufló coraje y heroísmo en los corazones de los jóvenes mártires de nuestra Fuerza Aérea.',\n",
       " 'No se equivocó el Comodoro Arturo Merino Benítez, primer Comandante en Jefe de la FACH, cuando en el funeral de uno de esos mártires -hace ya 89 años- afirmó “mañana, cuando sean realidad cotidiana los viajes aéreos a lo largo de nuestra República, y cuando recorran seguros y en confortables aviones que estarán mirando desde lo alto el árido y desolado desierto, la intrincada maraña de sus cerros, tal vez no recordarán cómo se ganó todo esto, a costa de qué esfuerzos, de qué abnegados sacrificios de unos muchachos valerosos, que quisieron y supieron vencer las dificultades, los peligros y la muerte”.']"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generamos un resumen de un texto aleatorio\n",
    "text = open('72186.txt', encoding='UTF-8').read()\n",
    "oraciones = PreProcesarTextos2([text])[0]\n",
    "resumen = generar_resumen(oraciones, 10, 0,0.2)\n",
    "resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('jupy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f53e49ab9d0d5819243b2b9e291675436d723b4e069eaa0e7e3fc4c332cba84a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
